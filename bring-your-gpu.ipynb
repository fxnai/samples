{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing your own GPU\n",
    "This predictor demonstrates how to bring your own GPU for making predictions on Function. Function integrates with [Runhouse](https://www.run.house/) to send your predictor code to your own custom GPU instances.\n",
    "\n",
    "> For startups, you'll find this especially useful if you have credits with the major cloud providers (e.g. AWS, Azure, GCP).\n",
    "\n",
    "> For enteprirse workloads, we instead recommend bringing your own cluster or running Function on-prem. [Reach out](mailto:sales@fxn.ai) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Runhouse\n",
    "First, we'll need to install `rsync`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y rsync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's install Runhouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Runhouse\n",
    "%pip install runhouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To login to RH, we'll need a Runhouse token. Head over to [your Runhouse account](https://www.run.house/account) to generate your RH token, then create a Function environment variable with your Runhouse token:\n",
    "```bash\n",
    "# Open a terminal and run the following command:\n",
    "fxn env create RH_TOKEN <Your Runhouse token>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Function Guard\n",
    "Before we write initialization code, we need to check whether a given block of code is executing in our Function predictor, or whether it is running on a Runhouse worker, which we create from the predictor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging into Runhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠹</span> Updating AWS catalog: aws/instance_quota_mapping.csv (every 7 hours)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m⠹\u001b[0m Updating AWS catalog: aws/instance_quota_mapping.csv (every 7 hours)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | 2023-09-25 12:52:58,271 | Saved secrets from Vault to local config files\n",
      "INFO | 2023-09-25 12:52:58,286 | Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO | 2023-09-25 12:52:59,315 | Uploaded secrets for to Vault for: ['aws', 'sky']\n",
      "INFO | 2023-09-25 12:52:59,316 | Successfully logged into Runhouse.\n"
     ]
    }
   ],
   "source": [
    "import runhouse as rh\n",
    "from os import environ\n",
    "\n",
    "rh.login(\n",
    "    token=environ[\"RH_TOKEN\"],\n",
    "    download_secrets=True,\n",
    "    download_config=True,\n",
    "    upload_config=True,\n",
    "    upload_secrets=True,\n",
    "    interactive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We've added a check to ensure that this code isn't executed on our Runhouse worker. It should only be executed in our Function predictor which creates the worker on Runhouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an On-Demand Cluster\n",
    "Now, we can create a cluster on our AWS account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance on AWS\n",
    "rh.cluster(\n",
    "    name=\"rh-a10x\",\n",
    "    instance_type=\"g5.2xlarge\",\n",
    "    provider=\"aws\",\n",
    "    autostop_mins=5,\n",
    "    region=\"us-east-1\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Predictor\n",
    "Let's create a predictor that uses StableDiffusion to generate an image from a text `prompt`. Unlike the standard `stable-diffusion` sample, here will be running the inference on a GPU in our own AWS account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | 2023-07-19 08:22:45,046 | Attempting to load config for /olokobayusuf/rh-a10x from RNS.\n"
     ]
    }
   ],
   "source": [
    "def stable_diffusion_generate (prompt: str):\n",
    "    # According to RH, imports must be inside the function\n",
    "    from diffusers import StableDiffusionPipeline\n",
    "    from torch import float16\n",
    "    # Create the SD pipeline\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-base\", torch_dtype=float16, revision=\"fp16\")\n",
    "    # Send to the GPU\n",
    "    pipeline.to(\"cuda\")\n",
    "    # Generate an image\n",
    "    result = pipeline(prompt).images[0]\n",
    "    # Return\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stable_diffusion_generate` function will be sent to our GPU instance for inference. With this, we can now write our Function predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our GPU cluster\n",
    "gpu = rh.cluster(name=\"rh-a10x\")\n",
    "\n",
    "# Send the `stable_diffusion_generate` function to our GPU cluster using RH\n",
    "stable_diffusion_generate = rh.function(fn=stable_diffusion_generate).to(\n",
    "    gpu,\n",
    "    env=[\"./\", \"torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117\", \"diffusers\", \"transformers\"]\n",
    ")\n",
    "\n",
    "# Define the Function predictor\n",
    "def predict (prompt: str):\n",
    "    # Generate an image by sending to our custom cluster\n",
    "    result = stable_diffusion_generate(prompt)\n",
    "    # Return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Predictor on Function\n",
    "Because we're bringing our own GPU, we can create the predictor on Function and use CPU acceleration:\n",
    "```bash\n",
    "fxn create @username/byo-gpu bring-your-gpu.ipynb --overwrite\n",
    "```\n",
    "\n",
    "> Replace `username` with your Function username."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
