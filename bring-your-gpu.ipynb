{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing your own GPU\n",
    "This predictor demonstrates how to bring your own GPU for making predictions on Function. Function integrates with [Runhouse](https://www.run.house/) to send your predictor code to your own custom GPU instances.\n",
    "\n",
    "> For startups, you'll find this especially useful if you have credits with the major cloud providers (e.g. AWS, Azure, GCP).\n",
    "\n",
    "> For enteprirse workloads, we instead recommend bringing your own cluster or running Function on-prem. [Reach out](mailto:sales@fxn.ai) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Runhouse\n",
    "First, we'll need to install `rsync`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y rsync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's install Runhouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Runhouse\n",
    "%pip install runhouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To login to RH, we'll need a Runhouse token. Head over to [your Runhouse account](https://www.run.house/account) to generate your RH token, then create a Function environment variable with your Runhouse token:\n",
    "```bash\n",
    "# Open a terminal and run the following command:\n",
    "fxn env create RH_TOKEN <Your Runhouse token>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging into Runhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "import runhouse as rh\n",
    "\n",
    "is_in_fxn = \"RH_TOKEN\" in environ\n",
    "\n",
    "if is_in_fxn:\n",
    "    rh.login(\n",
    "        token=environ[\"RH_TOKEN\"],\n",
    "        download_secrets=True,\n",
    "        download_config=True,\n",
    "        upload_config=True,\n",
    "        upload_secrets=True,\n",
    "        interactive=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We've added a check to ensure that this code isn't executed on our Runhouse worker. It should only be executed in our Function predictor which creates the worker on Runhouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an On-Demand Cluster\n",
    "Now, we can create a cluster on our AWS account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance on AWS\n",
    "gpu = rh.cluster(\n",
    "    name=\"rh-a10x\",\n",
    "    instance_type=\"g5.2xlarge\",\n",
    "    provider=\"aws\",\n",
    "    autostop_mins=5,\n",
    "    region=\"us-east-1\"\n",
    ").save() if is_in_fxn else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Predictor\n",
    "Let's create a predictor that uses StableDiffusion to generate an image from a text `prompt`. Unlike the standard `stable-diffusion` sample, here will be running the inference on a GPU in our own AWS account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_diffusion_generate (prompt: str):\n",
    "    from diffusers import StableDiffusionPipeline\n",
    "    from torch import float16\n",
    "    # Create the SD pipeline on the GPU in our AWS account!!\n",
    "    pipeline_name = \"stabilityai/stable-diffusion-2-base\"\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(pipeline_name)\n",
    "    pipeline.to(\"cuda\")\n",
    "    # Generate an image\n",
    "    result = pipeline(prompt).images[0]\n",
    "    # Return\n",
    "    return result\n",
    "\n",
    "if is_in_fxn:\n",
    "    # Send the `stable_diffusion_generate` function to our GPU cluster using RH\n",
    "    stable_diffusion_generate = rh.function(fn=stable_diffusion_generate).to(\n",
    "        gpu,\n",
    "        env=[\"./\", \"torch\", \"diffusers\", \"transformers\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stable_diffusion_generate` function will be sent to our GPU instance for inference. With this, we can now write our Function predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Function predictor\n",
    "def predict (prompt: str):\n",
    "    # Generate an image by sending to our custom cluster\n",
    "    result = stable_diffusion_generate(prompt)\n",
    "    # Return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Predictor on Function\n",
    "Because we're bringing our own GPU, we can create the predictor on Function and use CPU acceleration:\n",
    "```bash\n",
    "fxn create @username/byo-gpu bring-your-gpu.ipynb --overwrite\n",
    "```\n",
    "\n",
    "> Replace `username` with your Function username."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
